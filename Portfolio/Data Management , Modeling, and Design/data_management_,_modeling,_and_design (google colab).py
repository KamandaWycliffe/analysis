# -*- coding: utf-8 -*-
"""Data Management , Modeling, and Design.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wxZegem2xzSr4WuD_Ipa7qH21SQHpAos
"""

import tweepy
from google.colab import drive
#Mount drive
drive.mount('/content/drive/')

"""## A. Use a Twitter account* to register a developer app on https://apps.twitter.com, and select a hashtag to access data from for storage and analysis.

> We will use the #digitalhealth hashtag

## B. Perform historic data capture from the selected hashtag feed and store the data as a JSON file.
"""

# Python Script to Extract tweets of a 
# particular Hashtag using Tweepy and Pandas
  
  
# import modules
import pandas as pd
import tweepy
  
  
# function to display data of each tweet
def printtweetdata(n, ith_tweet):
    pass
  
  
# function to perform data extraction
def collecte(words, date_since, numtweet):
      
    # Creating DataFrame using pandas
    db = pd.DataFrame(columns=['username', 'description', 'location', 'following',
                               'followers','timestamp', 'mentions', 'totaltweets', 'retweetcount', 'text', 'hashtags'])
      
    # We are using .Cursor() to search through twitter for the required tweets.
    # The number of tweets can be restricted using .items(number of tweets)
    tweets = tweepy.Cursor(api.search, q=words, lang="en",
                           since=date_since, tweet_mode='extended').items(numtweet)
     
    # .Cursor() returns an iterable object. Each item in 
    # the iterator has various attributes that you can access to 
    # get information about each tweet
    list_tweets = [tweet for tweet in tweets]
      
    # Counter to maintain Tweet Count
    i = 1  
      
    # we will iterate over each tweet in the list for extracting information about each tweet
    for tweet in list_tweets:
        username = tweet.user.screen_name
        description = tweet.user.description
        location = tweet.user.location
        following = tweet.user.friends_count
        followers = tweet.user.followers_count
        timestamp = tweet.created_at
        user_mentions = tweet.entities['user_mentions']
        if user_mentions:
          mentions = [user['screen_name'] for user in user_mentions] # name, id, screen_name
        else:
          mentions = user_mentions
        totaltweets = tweet.user.statuses_count
        retweetcount = tweet.retweet_count
        hashtags = tweet.entities['hashtags']

          
        # Retweets can be distinguished by a retweeted_status attribute,
        # in case it is an invalid reference, except block will be executed
        try:
            text = tweet.retweeted_status.full_text
        except AttributeError:
            text = tweet.full_text
        hashtext = list()
        for j in range(0, len(hashtags)):
            hashtext.append(hashtags[j]['text'])
          
        # Here we are appending all the extracted information in the DataFrame
        ith_tweet = [username, description, location, following,
                     followers,timestamp,mentions, totaltweets, retweetcount, text, hashtext]
        db.loc[len(db)] = ith_tweet
          
        # Function call to print tweet data on screen
        printtweetdata(i, ith_tweet)
        i = i+1
    filename = 'collected_tweets.csv'
      
    # we will save our database as a CSV file.
    db.to_json('/content/drive/MyDrive/Machine learning/historical_tweets.json', orient='columns')
  
  
if __name__ == '__main__':
      
    # Enter your own credentials obtained 
    # from your developer account
    consumer_key = "Plf6N9ZtnkmnXeHa5qIgOJipt"
    consumer_secret = "W0IcerPxfVeSjMUlNTEL5FaDJ9HVHxikQV60N4ee1fnljjasI6"
    access_key = "818345262221590528-eduHlh4TPISFyueZWonEUgnHNcVVx1Y"
    access_secret = "XwwEApZw8OXIK5JJLoya6M5FkICoiR45UEqHj0D9phO9L"
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)
      
    # Enter Hashtag and initial date
    print("Enter Twitter HashTag to search for (exclude the # symbole for instance use digitalhealth instead of #digitalhealth")
    words = input()
    print("Enter Date since The Tweets are required in yyyy-mm--dd")
    date_since = input()
      
    # number of tweets you want to extract in one run
    print('Number of tweets to scrape')
    numtweet = int(input())
    collecte(words, date_since, numtweet)
    print('collecting has completed!')

import pandas as pd
#Read the collected json file
df = pd.read_json('/content/drive/MyDrive/Machine learning/historical_tweets.json')

df.head()

df.columns

import re
#1-Cleaning text column to remove user mentions from text column

clean_text = [re.sub('@[A-Za-z0-9]+', '', item) for item in df.text]

#2- Remove links from tweet text

clean_text = [re.sub(r"http\S+", "", subject) for subject in clean_text]

df['clean_text'] = clean_text
#3- Remove punctuations
df["clean_text"] = df['clean_text'].str.replace('[^\w\s]','')

import string
#Remove the square brackets by converting row lists to string
df['mentions'] = [','.join(item) for item in df['mentions']]


## 4- Remove stop words
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
#Remove stop words
df['clean_text']= df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))


#Extract hour and month from the data
#month
df['month'] = df.timestamp.dt.month
#hour
df['hour'] = df.timestamp.dt.hour

"""### Frequency of mentions of organisation(s) of your choice."""

dfx = df[['mentions']]
#Set splitting value to 50 to make sue we capture all the mentions
dfx = dfx['mentions'].str.split(",", n = 50, expand = True)

orgs = dfx.to_numpy().flatten().tolist()

#remove Nones
orgs = [x for x in orgs if x is not None]
dff = pd.DataFrame()
dff['orgs'] = orgs
#Group by to obtain count of each organization/person
orgs1 = pd.DataFrame(dff.groupby('orgs')['orgs'].count())
orgs1.columns = ['Count']
orgs1 = orgs1.reset_index()
#Define length column to help remove organizations with no names later
orgs1['len'] =  [len(item) for item in orgs1['orgs']]
orgs1 = orgs1[orgs1['len'] > 0]
#Drop the length column
orgs1 = orgs1.drop('len', axis = 1)
selectt = orgs1.sort_values('Count', ascending = False)
selectt1 = selectt[(selectt['orgs'] == 'NECA_TOYS') | (selectt['orgs'] == 'today_crypto')]

selectt1

"""### Frequency of selected meaningful text term(s)."""

#Split to obtain all the terms
term_df = df.clean_text.str.split(expand=True).stack().value_counts().reset_index()
#Get frequency of all terms
term_df.columns = ['Word', 'Frequency']
term_df['Word'] = [item.title() for item in term_df['Word']]
#Define meaningful test terms
terms = ['Machinelearning', 'Ai', 'Bigdata','Nfts',"Design", "Iot", 'Datamining', 'Covid', 'Covid19', 'Coronavirus', 'Corona']
 
key_terms = term_df[term_df['Word'].isin(terms)].reset_index()
key_terms = key_terms.drop('index', axis = 1)
key_terms = pd.DataFrame(key_terms.groupby('Word')['Frequency'].sum()).reset_index()
key_terms.sort_values('Frequency', ascending = False)

term_df.sort_values('Word', ascending = True)

"""### Most used @ mentions"""

dfx = df[['mentions']]
#Set splitting value to 50 to make sue we capture all the mentions
dfx = dfx['mentions'].str.split(",", n = 50, expand = True)

orgs = dfx.to_numpy().flatten().tolist()

#remove Nones
orgs = [x for x in orgs if x is not None]
dff = pd.DataFrame()
dff['orgs'] = orgs
#Group by to obtain count of each organization/person
orgs1 = pd.DataFrame(dff.groupby('orgs')['orgs'].count())
orgs1.columns = ['Count']
orgs1 = orgs1.reset_index()
#Define length column to help remove entities with no names later
orgs1['len'] =  [len(item) for item in orgs1['orgs']]
#Subset to remove entities that do not have names
orgs1 = orgs1[orgs1['len'] > 0]
#Drop the length column
orgs1 = orgs1.drop('len', axis = 1)
selectt = orgs1.sort_values('Count', ascending = False)
#Top 20 most used @ mentions
selectt.head(20)

"""## C. Perform analysis of streaming data from the selected hashtag feed (minimum of 10,000 real-time tweets), explaining the method and application

### Collect data
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import tweepy
# import csv
# from tweepy.streaming import StreamListener
# 
# #pass security information to variables
# consumer_key = "Plf6N9ZtnkmnXeHa5qIgOJipt"
# consumer_secret = "W0IcerPxfVeSjMUlNTEL5FaDJ9HVHxikQV60N4ee1fnljjasI6"
# access_key = "818345262221590528-eduHlh4TPISFyueZWonEUgnHNcVVx1Y"
# access_secret = "XwwEApZw8OXIK5JJLoya6M5FkICoiR45UEqHj0D9phO9L"
# 
# 
# #use variables to access twitter
# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
# auth.set_access_token(access_key, access_secret)
# api = tweepy.API(auth)
# 
# #create an object called 'TweetStreamer'
# 
# class TweetStreamer(StreamListener):
#     def __init__(self):
#       super().__init__()
#       self.counter = 0
#       #Define the number of tweets to stream (in this case 10000 tweets)
#       self.limit = 10000
# 
#     def on_status(self, status):
#         # Writing status data
#         with open('/content/drive/MyDrive/Machine learning/OutputStreaming1.csv', 'a') as f:
#             writer = csv.writer(f)
#             hashtags = status.entities['hashtags']
#             hashtext = list()
#             hashh = []
#             for j in range(0, len(hashtags)):
#                 hashtext.append(hashtags[j]['text'])
#             hashh.append(hashtext)
#             writer.writerow([status.author.screen_name, status.created_at,status.user.followers_count,hashtext, status.text])
#             self.counter += 1
#             if self.counter < self.limit:
#                 return True
#             else:
#                 streamingAPI.disconnect()
#             
# 
# 
#     def on_error(self, status_code):
#         print >> sys.stderr, 'Encountered error with status code:', status_code
#         return True # Don't kill the stream
# 
#     def on_timeout(self):
#         print >> sys.stderr, 'Timeout...'
#         return True # Don't kill the stream
# 
# runtime = 60 #this means one minute
# streamingAPI = tweepy.streaming.Stream(auth, TweetStreamer())
# print('Input the hashtag you would like to track such as design, healthcare, etc')
# hastagg = input()
# streamingAPI.filter(track=[hastagg])



import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/Machine learning/OutputStreaming.csv', header= None)
df.columns = ['user', 'created_at', 'followers_count','hashtag', 'text']
df.head(10)

"""###  A list of hashtags most used with your selected hashtag"""

import warnings
warnings.filterwarnings('ignore')
dfx = df[['hashtag']]
#Remove the square brackets by converting row lists to string
import string
def remove_punctuation(s):
    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])
    return s

dfx['hashtag'] = dfx['hashtag'].apply(remove_punctuation)
#Set splitting value to 50 to make sure we capture all the hashtags
dfx = dfx['hashtag'].str.split(" ", n = 50, expand = True)

hash = dfx.to_numpy().flatten().tolist()

#remove Nones
hash = [x for x in hash if x is not None]
dff = pd.DataFrame()
dff['Hastags'] = hash
#Group by to obtain count of each organization/person
hash1 = pd.DataFrame(dff.groupby('Hastags')['Hastags'].count())
hash1.columns = ['Count']
hash1 = hash1.reset_index()
#Define length column to help remove entities with no names later
hash1['len'] =  [len(item) for item in hash1['Hastags']]
#Subset to remove entities that do not have names
hash1 = hash1[hash1['len'] > 0]
#Drop the length column
hash1 = hash1.drop('len', axis = 1)
selectt = hash1.sort_values('Count', ascending = False)
#Top 20 most used @ mentions
selectt.head(20)

selectt

"""### Most occurring terms"""

import re
#1-Cleaning text column to remove user mentions from text column
clean_text = [re.sub('@[A-Za-z0-9]+', '', item) for item in df.text]

#2- Remove links from tweet text

clean_text = [re.sub(r"http\S+", "", subject) for subject in clean_text]

df['clean_text'] = clean_text
#3- Remove punctuations
df["clean_text"] = df['clean_text'].str.replace('[^\w\s]','')

## 4- Remove stop words
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

df['clean_text']= df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

#Split to obtain all the terms
term_df = df.clean_text.str.split(expand=True).stack().value_counts().reset_index()
#Get frequency of all terms
term_df.columns = ['Word', 'Frequency']
#Get length of words and assume that important words have more than 3 letters
term_df['len'] = [len(item) for item in term_df['Word']]
term_df = term_df[term_df['len'] > 3]
#Drop the length variable
term_df = term_df.drop('len', axis = 1)
term_df.head(20)

"""### Terms that may frequently occur together in the feed (bigram terms)"""

nltk.download('words')
words = set(nltk.corpus.words.words())
#Remove non-english words
df['cleantext'] = [" ".join(w for w in nltk.wordpunct_tokenize(item) if w.lower() in words or not w.isalpha()) for item in df['clean_text']]

from collections import Counter
from itertools import chain
#Define a function to generate the n-grams
def find_ngrams(input_list, n):
    return list(zip(*[input_list[i:] for i in range(n)]))

#Generate the bigrams by setting n to 2
df['bigrams'] = df['cleantext'].map(lambda x: find_ngrams(x.split(" "), 2))
# Bigram Frequency Counts
bigrams = df['bigrams'].tolist()
bigrams = list(chain(*bigrams))
bigrams = [(x.lower(), y.lower()) for x,y in bigrams]

bigram_counts = Counter(bigrams)
bigram_counts.most_common(20)

#add to dataframe
usage = pd.DataFrame(bigram_counts.most_common(20))
usage.columns = ['bigram', 'Number of times used']
usage